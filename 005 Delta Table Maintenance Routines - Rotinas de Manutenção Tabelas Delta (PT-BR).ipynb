{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0f6dfe-dc9e-4dec-aa59-8c8c9f05fe72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explanation of Maintenance Routines:\n",
    "- **Vacuum**: Removes old files to free up space.\n",
    "- **Optimize**: Combines small files to improve read performance.\n",
    "- **Z-Ordering**: Organizes data to improve performance in filtered queries.\n",
    "- **History/Time Travel**: Audits and accesses previous versions of data.\n",
    "\n",
    "### Explicação das Rotinas de Manutenção:\n",
    "- **Vacuum**: Remove arquivos antigos para liberar espaço.\n",
    "- **Optimize**: Combina arquivos pequenos para melhorar a performance de leitura.\n",
    "- **Z-Ordering**: Organiza os dados para melhorar o desempenho em consultas filtradas.\n",
    "- **History/Time Travel**: Audita e acessa versões anteriores dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce51ca10-cbb6-459c-aeb7-fa82b1276af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Maintaining a well-managed Delta Lake is essential to ensuring performance, data integrity, and efficient use of resources. Here are the main Delta Lake maintenance routines, when, how, and why to use them:\n",
    "\n",
    "### 1. Vacuum\n",
    "**When to use**: To remove old files that are no longer needed, such as those generated by update, merge, or delete operations.\n",
    "\n",
    "**Why to use**: Delta Lake maintains old versions of data (history) to provide features such as time travel and rollback. Over time, these old files can consume a lot of disk space. Vacuum removes these files, freeing up space.\n",
    "\n",
    "**Recommendation**: Avoid setting the retention period below 7 days without considering the implications for time travel. The default of 7 days is safe to maintain data recoverability while cleaning up obsolete files.\n",
    "\n",
    "\n",
    "Manter um** Delta Lake** bem gerenciado é fundamental para garantir a performance, a integridade dos dados e o uso eficiente de recursos. Aqui estão as principais rotinas de manutenção do Delta Lake, quando, como e por que usá-las:\n",
    "\n",
    "### 1. Vacuum\n",
    "**Quando usar**: Para remover arquivos antigos que não são mais necessários, como aqueles gerados por operações de update, merge ou delete.\n",
    "\n",
    "**Por que usar**: O Delta Lake mantém versões antigas de dados (histórico) para fornecer recursos como time travel e rollback. Com o tempo, esses arquivos antigos podem consumir muito espaço em disco. O vacuum remove esses arquivos, liberando espaço.\n",
    "\n",
    "**Recomendação**: Evite configurar o período de retenção abaixo de 7 dias sem considerar as implicações no time travel. O padrão de 7 dias é seguro para manter a possibilidade de recuperação de dados e, ao mesmo tempo, limpar arquivos obsoletos. (PT-BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf5c94e-5c83-4c84-bcc6-6d78a6bf2581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "gold_path = \"/mnt/lhdw/layer_gold/fato_acidentes/\"\n",
    "\n",
    "# Disable retention duration check\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# Run vacuum to remove unused files older than 7 days\n",
    "# Executa vacuum para remover arquivos não utilizados com mais de 7 dias (PT-BR)\n",
    "delta_table = DeltaTable.forPath(spark, gold_path)\n",
    "delta_table.vacuum(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e72ef92-402b-44e9-af99-a2f152f246d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. (Optimize)\n",
    "How to use ?: To optimize the layout of files stored in Delta Lake, especially after many write or update operations that can result in small files.\n",
    "\n",
    "Why use ?: Delta Lake can end up with many small files after write or merge operations. This can hurt query performance due to the overhead of reading many files. Optimize combines small files into larger files, improving read and processing.\n",
    "\n",
    "Recommendation: Use optimize at regular intervals or after large write operations to ensure that the data layout remains efficient. To further improve performance, optimize can be combined with Z-Ordering.\n",
    "\n",
    "### 2. (Optimize)\n",
    "como usar ?: Para otimizar o layout dos arquivos armazenados no Delta Lake, especialmente após muitas operações de escrita ou atualização que podem gerar arquivos pequenos.\n",
    "\n",
    "Por que usar ?: O Delta Lake pode acabar com muitos arquivos pequenos após operações de escrita ou merge. Isso pode prejudicar o desempenho das consultas devido ao overhead de leitura de muitos arquivos. O optimize combina arquivos pequenos em arquivos maiores, melhorando a leitura e o processamento.\n",
    "\n",
    "Recomendação: Use o optimize em intervalos regulares ou após grandes operações de escrita, para garantir que o layout dos dados continue eficiente. Para melhorar ainda mais o desempenho, o optimize pode ser combinado com Z-Ordering. (PT-BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca09092-9d19-498a-bc46-6714a5fccac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint>]"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "# Define os caminhos de armazenamento no Data Lake\n",
    "gold_path = \"/mnt/lhdw/layer_gold/fato_acidentes/\"\n",
    "# Otimiza a tabela combinando arquivos pequenos em arquivos maiores\n",
    "delta_table = DeltaTable.forPath(spark, gold_path)\n",
    "delta_table.optimize().executeCompaction()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60998b5d-532b-4cec-a82a-a12cb1c59564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 3. Z-Ordering\n",
    "When to use it?: To optimize queries that frequently filter on specific columns, such as date or key columns.\n",
    "\n",
    "Why to use it?: Z-Ordering improves read performance by physically organizing data on disk based on a column or set of columns, reducing the time required to fetch the filtered records.\n",
    "\n",
    "Recommended?: Use Z-Ordering on columns that are frequently used in filter clauses to improve readability of related data. Combine this with optimize to have more efficiently organized data on disk.\n",
    "\n",
    "\n",
    "### 3. Z-Ordering\n",
    "Quando usar ?:Para otimizar as consultas que realizam filtragens frequentes em determinadas colunas, como colunas de data ou chave.\n",
    "\n",
    "Por que usar?: O Z-Ordering melhora o desempenho da leitura ao organizar fisicamente os dados em disco com base em uma coluna ou conjunto de colunas, reduzindo o tempo necessário para buscar os registros filtrados.\n",
    "\n",
    "Recomendação?: Use o Z-Ordering em colunas que são frequentemente usadas em cláusulas de filtro para melhorar a leitura de dados relacionados. Combine isso com o optimize para ter dados organizados de forma mais eficiente no disco. (PT-BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1445b2ca-1544-4a88-a270-b64de321398a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "gold_path = \"/mnt/lhdw/layer_gold/fato_acidentes/\"\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "def vacuum_table(retention_hours=168):\n",
    "    \"\"\"\n",
    "    Remove arquivos antigos (default: 168 horas = 7 dias).\n",
    "    \"\"\"\n",
    "    print(f\"Executando VACUUM com retenção de {retention_hours} horas...\")\n",
    "    delta_table.vacuum(retention_hours)\n",
    "    print(\"Vacuum concluído.\")\n",
    "\n",
    "def optimize_table_zorder():\n",
    "    \"\"\"\n",
    "    Otimiza a tabela aplicando Z-Ordering pela coluna SK_tempo.\n",
    "    \"\"\"\n",
    "    print(\"Executando OPTIMIZE com Z-Ordering pela coluna SK_tempo...\")\n",
    "    spark.sql(f\"\"\"OPTIMIZE delta.`{gold_path}` ZORDER BY (SK_tempo)\"\"\")\n",
    "    print(\"Optimize com Z-Ordering concluído.\")\n",
    "\n",
    "def show_history():\n",
    "    \"\"\"\n",
    "    Exibe o histórico da tabela Delta (Time Travel).\n",
    "    \"\"\"\n",
    "    print(\"Histórico de transações:\")\n",
    "    history_df = delta_table.history()\n",
    "    display(history_df)\n",
    "\n",
    "def update_records(filter_condition, update_expr):\n",
    "    \"\"\"\n",
    "    Atualiza registros conforme condição.\n",
    "    Exemplo: update_records(\"Ano = 2020\", {\"mortos\": \"0\"})\n",
    "    \"\"\"\n",
    "    print(f\"Atualizando registros onde {filter_condition}...\")\n",
    "    delta_table.update(\n",
    "        condition=filter_condition,\n",
    "        set=update_expr\n",
    "    )\n",
    "    print(\"Update concluído.\")\n",
    "\n",
    "def delete_records(filter_condition):\n",
    "    \"\"\"\n",
    "    Remove registros conforme condição.\n",
    "    Exemplo: delete_records(\"ignorados > 0\")\n",
    "    \"\"\"\n",
    "    print(f\"Deletando registros onde {filter_condition}...\")\n",
    "    delta_table.delete(filter_condition)\n",
    "    print(\" Delete concluído.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144a9fe4-41d5-4a9d-8bd7-c77b68f9407d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5. History and Time Travel\n",
    "When to use it? : To audit changes to the Delta table or to access previous versions of the data.\n",
    "\n",
    "Why to use it? : Delta Lake maintains a transaction log that allows you to track all modifications made to the table. This is useful for auditing and recovering data to a previous point in time.\n",
    "\n",
    "Recommended? : Use history and time travel to debug issues or restore previous versions of the data when necessary. However, remember to use vacuum to manage the amount of history kept.\n",
    "\n",
    "### 5. History e Time Travel\n",
    "Quando usar? : Para auditar mudanças na tabela Delta ou para acessar versões anteriores dos dados.\n",
    "\n",
    "Por que usar ? : O Delta Lake mantém um log de transações que permite rastrear todas as modificações feitas na tabela. Isso é útil para auditoria e recuperação de dados em um ponto anterior no tempo.\n",
    "\n",
    "Recomendação? : Use o histórico e o time travel para depurar problemas ou restaurar versões anteriores dos dados quando necessário. No entanto, lembre-se de usar o vacuum para gerenciar a quantidade de histórico mantido. (PT-BR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254a5b63-4140-416b-ad34-1c67b01efc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Accessing table history:\n",
    "\n",
    "### Acessando o histórico da tabela: (PT-BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ae9ecd-4a94-45f4-916a-aad8dd0845a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>19</td><td>2025-05-21T02:19:18.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [], batchId -> 0, auto -> false)</td><td>null</td><td>List(1870597478109446)</td><td>0521-005749-9mxgmz85</td><td>18</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 6, numRemovedBytes -> 485315, p25FileSize -> 464457, numDeletionVectorsRemoved -> 0, minFileSize -> 464457, numAddedFiles -> 1, maxFileSize -> 464457, p75FileSize -> 464457, p50FileSize -> 464457, numAddedBytes -> 464457)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>18</td><td>2025-05-21T01:44:47.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0521-005749-9mxgmz85</td><td>17</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485315)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>17</td><td>2025-05-19T13:52:58.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0519-132418-r7rmca2x</td><td>16</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485631)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>16</td><td>2025-05-19T11:33:04.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>RESTORE</td><td>Map(version -> 2, timestamp -> null)</td><td>null</td><td>List(1870597478109446)</td><td>0519-110908-k93zsuug</td><td>15</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 6, removedFilesSize -> 462795, numRemovedFiles -> 1, restoredFilesSize -> 485550, numOfFilesAfterRestore -> 6, tableSizeAfterRestore -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>15</td><td>2025-05-19T11:32:13.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [], batchId -> 0, auto -> false)</td><td>null</td><td>List(1870597478109446)</td><td>0519-110908-k93zsuug</td><td>14</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 6, numRemovedBytes -> 485854, p25FileSize -> 462795, numDeletionVectorsRemoved -> 0, minFileSize -> 462795, numAddedFiles -> 1, maxFileSize -> 462795, p75FileSize -> 462795, p50FileSize -> 462795, numAddedBytes -> 462795)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>14</td><td>2025-05-19T11:20:30.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0519-110908-k93zsuug</td><td>13</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485854)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>13</td><td>2025-05-18T15:14:29.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>RESTORE</td><td>Map(version -> 2, timestamp -> null)</td><td>null</td><td>List(1870597478109446)</td><td>0518-141425-3w2ti7p4</td><td>12</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 6, removedFilesSize -> 463120, numRemovedFiles -> 1, restoredFilesSize -> 485550, numOfFilesAfterRestore -> 6, tableSizeAfterRestore -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>12</td><td>2025-05-18T15:13:39.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [], batchId -> 0, auto -> false)</td><td>null</td><td>List(1870597478109446)</td><td>0518-141425-3w2ti7p4</td><td>11</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 6, numRemovedBytes -> 484981, p25FileSize -> 463120, numDeletionVectorsRemoved -> 0, minFileSize -> 463120, numAddedFiles -> 1, maxFileSize -> 463120, p75FileSize -> 463120, p50FileSize -> 463120, numAddedBytes -> 463120)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>11</td><td>2025-05-18T15:08:36.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0518-141425-3w2ti7p4</td><td>10</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 484981)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>10</td><td>2025-05-18T14:50:21.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>RESTORE</td><td>Map(version -> 2, timestamp -> null)</td><td>null</td><td>List(1870597478109446)</td><td>0518-141425-3w2ti7p4</td><td>9</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 6, removedFilesSize -> 463938, numRemovedFiles -> 1, restoredFilesSize -> 485550, numOfFilesAfterRestore -> 6, tableSizeAfterRestore -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>9</td><td>2025-05-18T14:49:30.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [], batchId -> 0, auto -> false)</td><td>null</td><td>List(1870597478109446)</td><td>0518-141425-3w2ti7p4</td><td>8</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 6, numRemovedBytes -> 485537, p25FileSize -> 463938, numDeletionVectorsRemoved -> 0, minFileSize -> 463938, numAddedFiles -> 1, maxFileSize -> 463938, p75FileSize -> 463938, p50FileSize -> 463938, numAddedBytes -> 463938)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>8</td><td>2025-05-18T14:43:55.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0518-141425-3w2ti7p4</td><td>7</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485537)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>7</td><td>2025-05-18T14:39:54.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0518-141425-3w2ti7p4</td><td>6</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485537)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>6</td><td>2025-05-18T14:36:04.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0518-141425-3w2ti7p4</td><td>5</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485537)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>5</td><td>2025-05-18T14:32:44.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0518-141425-3w2ti7p4</td><td>4</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485537)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>4</td><td>2025-05-18T04:51:24.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>RESTORE</td><td>Map(version -> 2, timestamp -> null)</td><td>null</td><td>List(1870597478109446)</td><td>0517-232140-ovufdw7c</td><td>3</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 6, removedFilesSize -> 462822, numRemovedFiles -> 1, restoredFilesSize -> 485550, numOfFilesAfterRestore -> 6, tableSizeAfterRestore -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>3</td><td>2025-05-18T04:45:29.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], zOrderBy -> [], batchId -> 0, auto -> false)</td><td>null</td><td>List(1870597478109446)</td><td>0517-232140-ovufdw7c</td><td>2</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 6, numRemovedBytes -> 485550, p25FileSize -> 462822, numDeletionVectorsRemoved -> 0, minFileSize -> 462822, numAddedFiles -> 1, maxFileSize -> 462822, p75FileSize -> 462822, p50FileSize -> 462822, numAddedBytes -> 462822)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>2</td><td>2025-05-18T04:36:02.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0517-232140-ovufdw7c</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>1</td><td>2025-05-18T04:33:07.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0517-232140-ovufdw7c</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr><tr><td>0</td><td>2025-05-18T04:31:26.000+0000</td><td>3991693015469206</td><td>clay_cloud@hotmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, partitionBy -> [])</td><td>null</td><td>List(4338771034993245)</td><td>0517-232140-ovufdw7c</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 6, numOutputRows -> 16715, numOutputBytes -> 485550)</td><td>null</td><td>Databricks-Runtime/12.2.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         19,
         "2025-05-21T02:19:18.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1870597478109446"
         ],
         "0521-005749-9mxgmz85",
         18,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "464457",
          "minFileSize": "464457",
          "numAddedBytes": "464457",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "485315",
          "numRemovedFiles": "6",
          "p25FileSize": "464457",
          "p50FileSize": "464457",
          "p75FileSize": "464457"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         18,
         "2025-05-21T01:44:47.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0521-005749-9mxgmz85",
         17,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485315",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         17,
         "2025-05-19T13:52:58.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0519-132418-r7rmca2x",
         16,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485631",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         16,
         "2025-05-19T11:33:04.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "2"
         },
         null,
         [
          "1870597478109446"
         ],
         "0519-110908-k93zsuug",
         15,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "6",
          "numRemovedFiles": "1",
          "numRestoredFiles": "6",
          "removedFilesSize": "462795",
          "restoredFilesSize": "485550",
          "tableSizeAfterRestore": "485550"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         15,
         "2025-05-19T11:32:13.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1870597478109446"
         ],
         "0519-110908-k93zsuug",
         14,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "462795",
          "minFileSize": "462795",
          "numAddedBytes": "462795",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "485854",
          "numRemovedFiles": "6",
          "p25FileSize": "462795",
          "p50FileSize": "462795",
          "p75FileSize": "462795"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         14,
         "2025-05-19T11:20:30.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0519-110908-k93zsuug",
         13,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485854",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         13,
         "2025-05-18T15:14:29.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "2"
         },
         null,
         [
          "1870597478109446"
         ],
         "0518-141425-3w2ti7p4",
         12,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "6",
          "numRemovedFiles": "1",
          "numRestoredFiles": "6",
          "removedFilesSize": "463120",
          "restoredFilesSize": "485550",
          "tableSizeAfterRestore": "485550"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         12,
         "2025-05-18T15:13:39.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1870597478109446"
         ],
         "0518-141425-3w2ti7p4",
         11,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "463120",
          "minFileSize": "463120",
          "numAddedBytes": "463120",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "484981",
          "numRemovedFiles": "6",
          "p25FileSize": "463120",
          "p50FileSize": "463120",
          "p75FileSize": "463120"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         11,
         "2025-05-18T15:08:36.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0518-141425-3w2ti7p4",
         10,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "484981",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         10,
         "2025-05-18T14:50:21.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "2"
         },
         null,
         [
          "1870597478109446"
         ],
         "0518-141425-3w2ti7p4",
         9,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "6",
          "numRemovedFiles": "1",
          "numRestoredFiles": "6",
          "removedFilesSize": "463938",
          "restoredFilesSize": "485550",
          "tableSizeAfterRestore": "485550"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         9,
         "2025-05-18T14:49:30.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1870597478109446"
         ],
         "0518-141425-3w2ti7p4",
         8,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "463938",
          "minFileSize": "463938",
          "numAddedBytes": "463938",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "485537",
          "numRemovedFiles": "6",
          "p25FileSize": "463938",
          "p50FileSize": "463938",
          "p75FileSize": "463938"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         8,
         "2025-05-18T14:43:55.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0518-141425-3w2ti7p4",
         7,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485537",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         7,
         "2025-05-18T14:39:54.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0518-141425-3w2ti7p4",
         6,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485537",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         6,
         "2025-05-18T14:36:04.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0518-141425-3w2ti7p4",
         5,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485537",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         5,
         "2025-05-18T14:32:44.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0518-141425-3w2ti7p4",
         4,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485537",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         4,
         "2025-05-18T04:51:24.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "2"
         },
         null,
         [
          "1870597478109446"
         ],
         "0517-232140-ovufdw7c",
         3,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "6",
          "numRemovedFiles": "1",
          "numRestoredFiles": "6",
          "removedFilesSize": "462822",
          "restoredFilesSize": "485550",
          "tableSizeAfterRestore": "485550"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         3,
         "2025-05-18T04:45:29.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "predicate": "[]",
          "zOrderBy": "[]"
         },
         null,
         [
          "1870597478109446"
         ],
         "0517-232140-ovufdw7c",
         2,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "462822",
          "minFileSize": "462822",
          "numAddedBytes": "462822",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "485550",
          "numRemovedFiles": "6",
          "p25FileSize": "462822",
          "p50FileSize": "462822",
          "p75FileSize": "462822"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         2,
         "2025-05-18T04:36:02.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0517-232140-ovufdw7c",
         1,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485550",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         1,
         "2025-05-18T04:33:07.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0517-232140-ovufdw7c",
         0,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485550",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ],
        [
         0,
         "2025-05-18T04:31:26.000+0000",
         "3991693015469206",
         "clay_cloud@hotmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]"
         },
         null,
         [
          "4338771034993245"
         ],
         "0517-232140-ovufdw7c",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "6",
          "numOutputBytes": "485550",
          "numOutputRows": "16715"
         },
         null,
         "Databricks-Runtime/12.2.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Visualizar o histórico da tabela Delta\n",
    "df_historico = DeltaTable.forPath(spark, \"/mnt/lhdw/layer_gold/fato_acidentes\").history()\n",
    "\n",
    "display(df_historico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9432069-a818-4938-9a60-5391f522c001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Access old versions (Time Travel):\n",
    "\n",
    "### Acessar versões antigas (Time Travel): (PT-BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa738c28-98f2-44f9-968d-924c9ebf9f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 489.0 failed 1 times, most recent failure: Lost task 0.0 in stage 489.0 (TID 1900) (ip-10-172-161-226.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:879)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:79)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:78)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n",
       "\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n",
       "\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:363)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:52)\n",
       "\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:79)\n",
       "\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:362)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:381)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$UnifiedCache.read(CachingParquetFooterReader.java:417)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:284)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:280)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:206)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:410)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:421)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:276)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:617)\n",
       "\t... 21 more\n",
       "Caused by: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:76)\n",
       "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:76)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\t... 38 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:727)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:887)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:286)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:879)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:79)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:78)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n",
       "\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n",
       "\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:363)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:52)\n",
       "\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:79)\n",
       "\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:362)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:381)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$UnifiedCache.read(CachingParquetFooterReader.java:417)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:284)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:280)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:206)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:410)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:421)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:276)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:617)\n",
       "\t... 21 more\n",
       "Caused by: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:76)\n",
       "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:76)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\t... 38 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 489.0 failed 1 times, most recent failure: Lost task 0.0 in stage 489.0 (TID 1900) (ip-10-172-161-226.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:879)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:79)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:78)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:363)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:52)\n\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:79)\n\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:362)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:381)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$UnifiedCache.read(CachingParquetFooterReader.java:417)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:284)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:280)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:206)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:410)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:421)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:276)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:617)\n\t... 21 more\nCaused by: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:76)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:76)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\t... 38 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:727)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.computeListResultsItem(JupyterDriverLocal.scala:887)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal$JupyterEntryPoint.addCustomDisplayData(JupyterDriverLocal.scala:286)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:879)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:79)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:78)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.util.concurrent.ExecutionException: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n\tat java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)\n\tat java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:363)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:52)\n\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:79)\n\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:362)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:381)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader$UnifiedCache.read(CachingParquetFooterReader.java:417)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$0(CachingParquetFooterReader.java:284)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:17)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:280)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:206)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:410)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:157)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:421)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:276)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:617)\n\t... 21 more\nCaused by: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:76)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:76)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\t... 38 more\n",
       "errorSummary": "FileReadException: Error while reading file dbfs:/mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet.\nCaused by: ExecutionException: java.io.FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet\nCaused by: FileNotFoundException: /mnt/lhdw/layer_gold/fato_acidentes/part-00001-1d4e4005-7f5f-4b55-ab65-0374803435cd-c000.snappy.parquet",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Acessar a versão 5 da tabela\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"/mnt/lhdw/layer_gold/fato_acidentes\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf762956-9d1e-4427-80f8-80ad52e92a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Restore an old version of Delta table\n",
    "### 6. Restaurar uma versão antiga de tabela Delta (PT_BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157a3238-e805-4e59-a033-41174f1f86d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta_table_path = \"dbfs:/mnt/lhdw/layer_gold/fato_acidentes\"\n",
    "\n",
    "# Verifica se é uma tabela Delta válida antes de continuar\n",
    "display(dbutils.fs.ls(f\"{delta_table_path}/_delta_log\"))\n",
    "\n",
    "# Registrar tabela temporária e restaurar\n",
    "spark.sql(\"DROP TABLE IF EXISTS temp_restore\")\n",
    "spark.sql(f\"CREATE TABLE temp_restore USING DELTA LOCATION '{delta_table_path}'\")\n",
    "spark.sql(\"RESTORE TABLE temp_restore TO VERSION AS OF 2\")\n",
    "\n",
    "# Exibir dados restaurados\n",
    "display(spark.read.format(\"delta\").load(delta_table_path))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1833114856513083,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "005 Delta Table Maintenance Routines - Rotinas de Manutenção Tabelas Delta (PT-BR)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}